{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yolov4.tf import YOLOv4\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildActionDict():\n",
    "    with open(\"ava_videos/action_list.pbtxt\", 'r') as file:\n",
    "        actions = file.read()\n",
    "    actions = actions.split('item {\\n  ')[1:]\n",
    "    actions = [[keys.split(': ') for keys in ac.split('\\n')[:2]] for ac in actions]\n",
    "    actions_dict ={}\n",
    "    for ac in actions:\n",
    "        actions_dict[int(ac[1][1])] = ac[0][1][1:-1]\n",
    "    return actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthBbox(df):\n",
    "    bboxes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        bboxes.append([row['x1'], row['y1'], row['x2'], row['y2'], row['action_id']])\n",
    "    return np.array(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_groundTruth_bboxes(image, bboxes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    actions = buildActionDict()\n",
    "    for bbox in bboxes:\n",
    "        top_left = (int(bbox[0]), int(bbox[1]))\n",
    "        bottom_right = (int(bbox[2]), int(bbox[3]))\n",
    "        action_id = bbox[4]\n",
    "        bbox_color = (255, 0, 255)\n",
    "        font_size = 0.4\n",
    "        font_thickness = 1\n",
    "        cv2.rectangle(image, top_left, bottom_right, bbox_color, 2)\n",
    "        bbox_text = actions[action_id]\n",
    "        t_size = cv2.getTextSize(bbox_text, 0, font_size, font_thickness)[0]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            top_left,\n",
    "            (top_left[0] + t_size[0], top_left[1] - t_size[1] - 3),\n",
    "            bbox_color,\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            bbox_text,\n",
    "            (top_left[0], top_left[1] - 2),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_size,\n",
    "            (255 - bbox_color[0], 255 - bbox_color[1], 255 - bbox_color[2]),\n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects(image, bboxes, classes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes.view('i8,i8,i8,i8,i8,i8').sort(order=['f0','f1'], axis=0)\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    person_count = 0\n",
    "    for bbox in bboxes:\n",
    "        c_x = int(bbox[0])\n",
    "        c_y = int(bbox[1])\n",
    "        half_w = int(bbox[2] / 2)\n",
    "        half_h = int(bbox[3] / 2)\n",
    "        top_left = [c_x - half_w, c_y - half_h]\n",
    "        bottom_right = [c_x + half_w, c_y + half_h]\n",
    "        top_left[0] = max(top_left[0], 0)\n",
    "        top_left[1] = max(top_left[1], 0)\n",
    "        bottom_right[0] = min(bottom_right[0], width)\n",
    "        bottom_right[1] = min(bottom_right[1], height)\n",
    "        class_id = int(bbox[4])\n",
    "        if class_id == 0:\n",
    "            person_count += 1\n",
    "            windowName = \"{}_{}\".format(classes[class_id],person_count)\n",
    "            cv2.namedWindow(windowName, cv2.WINDOW_AUTOSIZE)\n",
    "            obj = image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0], :]\n",
    "            cv2.imshow(windowName, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildYoloModel():\n",
    "    yolo = YOLOv4()\n",
    "    yolo.classes = \"coco.names\"\n",
    "    yolo.input_size=(608,608)\n",
    "    yolo.make_model()\n",
    "    yolo.load_weights(\"yolov4.weights\", weights_type='yolo')\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(media_path, yolo, groundTruth_df, iou_threshold = 0.3, score_threshold = 0.4, start_time = 902, end_time = 1798):\n",
    "    \n",
    "    if not os.path.exists(media_path):\n",
    "        raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
    "\n",
    "    cv2.namedWindow(\"result\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"origin\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"ground_truth\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    cap = cv2.VideoCapture(media_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            try:\n",
    "                is_success, frame = cap.read()\n",
    "            except cv2.error:\n",
    "                continue\n",
    "                \n",
    "            now_second = cap.get(0)/1000\n",
    "            \n",
    "            if now_second < start_time: continue\n",
    "            if (not is_success) or (now_second >= end_time+1): break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            bboxes = yolo.predict(\n",
    "                frame,\n",
    "                iou_threshold=iou_threshold,\n",
    "                score_threshold=score_threshold,\n",
    "            )\n",
    "            \n",
    "            groundTruth_bboxes = getGroundTruthBbox(groundTruth_df[groundTruth_df['timestamp']==int(now_second)])\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            image = yolo.draw_bboxes(frame, bboxes)\n",
    "            groundTruth_img = draw_groundTruth_bboxes(frame, groundTruth_bboxes)\n",
    "\n",
    "            cv2.imshow(\"result\", image)\n",
    "            cv2.imshow(\"origin\", frame)\n",
    "            cv2.imshow(\"ground_truth\", groundTruth_img)\n",
    "            #draw_objects(frame, bboxes, yolo.classes)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ava_videos/ava_file_names_trainval_v2.1.txt', 'r') as f:\n",
    "    video_names = f.readlines()\n",
    "video_names = [v.rstrip().split('.') for v in video_names]\n",
    "video_names_dict = {}\n",
    "for video in video_names:\n",
    "    video_names_dict[video[0]] = video[0]+'.'+video[1]\n",
    "\n",
    "columns = ['video_id', 'timestamp', 'x1', 'y1', 'x2', 'y2', 'action_id', 'person_id']\n",
    "train_df = pd.read_csv('ava_videos/ava_train_v2.2.csv')\n",
    "val_df = pd.read_csv('ava_videos/ava_val_v2.2.csv')\n",
    "train_df.columns = columns\n",
    "val_df.columns = columns\n",
    "train_df.drop(train_df[train_df['video_id']=='#NAME?'].index, inplace=True)\n",
    "train_df['video_id'] = train_df['video_id'].map(video_names_dict)\n",
    "val_df['video_id'] = val_df['video_id'].map(video_names_dict)\n",
    "\n",
    "train_videos = train_df['video_id'].unique()\n",
    "val_videos = val_df['video_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = buildYoloModel()\n",
    "train_path = \"ava_videos/train/\"\n",
    "val_path = \"ava_videos/val/\"\n",
    "media_name = train_videos[1]\n",
    "media_path = train_path + media_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-63d9660666ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedia_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmedia_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-181-a247d70be5ff>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(media_path, yolo, groundTruth_df, iou_threshold, score_threshold, start_time, end_time)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/yolov4/tf/__init__.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, frame, iou_threshold, score_threshold)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Select 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(media_path, yolo, train_df[train_df['video_id']==media_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
