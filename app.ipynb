{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yolov4.tf import YOLOv4\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.tools.generate_detections import create_box_encoder\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker as ds_Tracker\n",
    "MODEL_CKPT = \"./deep_sort/weights/mars-small128.pb\"\n",
    "import action_detection.action_detector as act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker():\n",
    "    def __init__(self, timesteps=32):\n",
    "        self.active_actors = []\n",
    "        self.inactive_actors = []\n",
    "        self.actor_no = 0\n",
    "        self.frame_history = []\n",
    "        self.frame_no = 0\n",
    "        self.timesteps = timesteps\n",
    "        self.actor_infos = {}\n",
    "        # deep sort\n",
    "        self.encoder = create_box_encoder(MODEL_CKPT, batch_size=16)\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.2, None) #, max_cosine_distance=0.2) #, nn_budget=None)\n",
    "        #self.tracker = ds_Tracker(metric, max_iou_distance=0.7, max_age=30, n_init=3)\n",
    "        #self.tracker = ds_Tracker(metric, max_iou_distance=0.7, max_age=200, n_init=1)\n",
    "        self.tracker = ds_Tracker(metric, max_iou_distance=0.7, max_age=200, n_init=5)\n",
    "        self.score_th = 0.40\n",
    "        #self.results = []\n",
    "\n",
    "\n",
    "    def update_tracker(self, detection_info, frame):\n",
    "        ''' Takes the frame and the results from the object detection\n",
    "            Updates the tracker wwith the current detections and creates new tracks\n",
    "        '''\n",
    "\n",
    "        boxes = np.array([d[:4] for d in detection_info])\n",
    "        classes = np.array([d[4] for d in detection_info])\n",
    "        scores = np.array([d[5] for d in detection_info])\n",
    "        num_detections = len(detection_info)\n",
    "        indices = np.logical_and(scores > self.score_th, classes == 0)# filter score threshold and non-person detections\n",
    "        filtered_boxes, filtered_scores = boxes[indices], scores[indices]\n",
    "\n",
    "        H,W,C = frame.shape\n",
    "        filtered_boxes[:, [0, 2]] = filtered_boxes[:, [0, 2]] * W\n",
    "        filtered_boxes[:, [1, 3]] = filtered_boxes[:, [1, 3]] * H\n",
    "        # deep sort format boxes (x, y, W, H)\n",
    "        ds_boxes = []\n",
    "        for bb in range(filtered_boxes.shape[0]):\n",
    "            cur_box = filtered_boxes[bb]\n",
    "            cur_score = filtered_scores[bb]\n",
    "            c_x = int(cur_box[0])\n",
    "            c_y = int(cur_box[1])\n",
    "            half_w = int(cur_box[2]/2)\n",
    "            half_h = int(cur_box[3]/2)\n",
    "            ds_box = [c_x - half_w, c_y - half_h, int(cur_box[2]), int(cur_box[3])]\n",
    "            ds_boxes.append(ds_box)\n",
    "        features = self.encoder(frame, ds_boxes)\n",
    "\n",
    "        detection_list = []\n",
    "        for bb in range(filtered_boxes.shape[0]):\n",
    "            cur_box = filtered_boxes[bb]\n",
    "            cur_score = filtered_scores[bb]\n",
    "            feature = features[bb]\n",
    "            c_x = int(cur_box[0])\n",
    "            c_y = int(cur_box[1])\n",
    "            half_w = int(cur_box[2]/2)\n",
    "            half_h = int(cur_box[3]/2)\n",
    "            ds_box = [c_x - half_w, c_y - half_h, int(cur_box[2]), int(cur_box[3])]\n",
    "            detection_list.append(Detection(ds_box, cur_score, feature))\n",
    "\n",
    "        # update tracker\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(detection_list)\n",
    "        \n",
    "        # Store results.\n",
    "        #results = []\n",
    "        actives = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlwh()\n",
    "            left, top, width, height = bbox\n",
    "            tr_box = [top / float(H), left / float(W), (top+height)/float(H), (left+width)/float(W)]\n",
    "            actor_id = track.track_id\n",
    "            detection_conf = track.last_detection_confidence\n",
    "            #results.append([frame_idx, track.track_id, bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "            #results.append({'all_boxes': [tr_box], 'all_scores': [1.00], 'actor_id': track.track_id})\n",
    "            if actor_id in self.actor_infos: # update with the new bbox info\n",
    "                cur_actor = self.actor_infos[actor_id]\n",
    "                no_interpolate_frames = self.frame_no - cur_actor['last_updated_frame_no']\n",
    "                interpolated_box_list = bbox_interpolate(cur_actor['all_boxes'][-1], tr_box, no_interpolate_frames)\n",
    "                cur_actor['all_boxes'].extend(interpolated_box_list[1:])\n",
    "                cur_actor['last_updated_frame_no'] = self.frame_no\n",
    "                cur_actor['length'] = len(cur_actor['all_boxes'])\n",
    "                cur_actor['all_scores'].append(detection_conf)\n",
    "                actives.append(cur_actor)\n",
    "            else:\n",
    "                new_actor = {'all_boxes': [tr_box], 'length':1, 'last_updated_frame_no': self.frame_no, 'all_scores':[detection_conf], 'actor_id':actor_id}\n",
    "                self.actor_infos[actor_id] = new_actor\n",
    "\n",
    "        self.active_actors = actives\n",
    "        \n",
    "        self.frame_history.append(frame)\n",
    "        if len(self.frame_history) > 2*self.timesteps:\n",
    "            del self.frame_history[0]\n",
    "\n",
    "        self.frame_no += 1\n",
    "\n",
    "    def generate_all_rois(self):\n",
    "        no_actors = len(self.active_actors)\n",
    "        rois_np = np.zeros([no_actors, 4])\n",
    "        temporal_rois_np = np.zeros([no_actors, self.timesteps, 4])\n",
    "        for bb, actor_info in enumerate(self.active_actors):\n",
    "            actor_no = actor_info['actor_id']\n",
    "            norm_roi, full_roi = self.generate_person_tube_roi(actor_no)\n",
    "            rois_np[bb] = norm_roi\n",
    "            temporal_rois_np[bb] = full_roi\n",
    "        return rois_np, temporal_rois_np\n",
    "\n",
    "    def generate_person_tube_roi(self, actor_id):\n",
    "        actor_info = [act for act in self.active_actors if act['actor_id'] == actor_id][0]\n",
    "        boxes = actor_info['all_boxes']\n",
    "        if actor_info['length'] < self.timesteps:\n",
    "            recent_boxes = boxes\n",
    "            index_offset = (self.timesteps - actor_info['length'] + 1) \n",
    "        else:\n",
    "            recent_boxes = boxes[-self.timesteps:]\n",
    "            index_offset = 0\n",
    "        H,W,C = self.frame_history[-1].shape\n",
    "        mid_box = recent_boxes[len(recent_boxes)//2]\n",
    "        # top, left, bottom, right = mid_box\n",
    "        # edge = max(bottom - top, right - left) / 2.\n",
    "        edge, norm_roi = generate_edge_and_normalized_roi(mid_box)\n",
    "\n",
    "        # tube = np.zeros([self.timesteps] + list(box_size) + [3], np.uint8)\n",
    "        full_rois = []\n",
    "        # for rr in range(len(recent_boxes)):\n",
    "        for rr in range(self.timesteps):\n",
    "            if rr < index_offset:\n",
    "                cur_box = recent_boxes[0]\n",
    "            else:\n",
    "                cur_box = recent_boxes[rr - index_offset]\n",
    "            \n",
    "            # zero pad so that we dont have to worry about edge cases\n",
    "            # cur_frame = self.frame_history[rr]\n",
    "            # padsize = int(edge * max(H,W))\n",
    "            # cur_frame = np.pad(cur_frame, [(padsize,padsize),(padsize,padsize), (0,0)], 'constant')\n",
    "\n",
    "            top, left, bottom, right = cur_box\n",
    "            cur_center = (top+bottom)/2., (left+right)/2.\n",
    "            top, bottom = cur_center[0] - edge, cur_center[0] + edge\n",
    "            left, right = cur_center[1] - edge, cur_center[1] + edge\n",
    "\n",
    "            # top_ind, bottom_ind = int(top * H)+padsize, int(bottom * H)+padsize\n",
    "            # left_ind, right_ind = int(left * W)+padsize, int(right * W)+padsize\n",
    "            # cur_image_crop = cur_frame[top_ind:bottom_ind, left_ind:right_ind]\n",
    "            # tube[rr+index_offset,:,:,:] = cv2.resize(cur_image_crop, box_size)\n",
    "            full_rois.append([top, left, bottom, right])\n",
    "        full_rois_np = np.stack(full_rois, axis=0)\n",
    "\n",
    "        return norm_roi, full_rois_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_interpolate(start_box, end_box, no_interpolate_frames):\n",
    "    delta = (np.array(end_box) - np.array(start_box)) / float(no_interpolate_frames)\n",
    "    interpolated_boxes = []\n",
    "    for ii in range(0, no_interpolate_frames+1):\n",
    "        cur_box = np.array(start_box) + delta * ii\n",
    "        interpolated_boxes.append(cur_box.tolist())\n",
    "    return interpolated_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_and_normalized_roi(mid_box):\n",
    "    top, left, bottom, right = mid_box\n",
    "\n",
    "    edge = max(bottom - top, right - left) / 2. * 1.5 # change this to change the size of the tube\n",
    "\n",
    "    cur_center = (top+bottom)/2., (left+right)/2.\n",
    "    context_top, context_bottom = cur_center[0] - edge, cur_center[0] + edge\n",
    "    context_left, context_right = cur_center[1] - edge, cur_center[1] + edge\n",
    "\n",
    "    normalized_top = (top - context_top) / (2*edge)\n",
    "    normalized_bottom = (bottom - context_top) / (2*edge)\n",
    "\n",
    "    normalized_left = (left - context_left) / (2*edge)\n",
    "    normalized_right = (right - context_left) / (2*edge)\n",
    "\n",
    "    norm_roi = [normalized_top, normalized_left, normalized_bottom, normalized_right]\n",
    "\n",
    "    return edge, norm_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildActionDict():\n",
    "    with open(\"ava_videos/action_list.pbtxt\", 'r') as file:\n",
    "        actions = file.read()\n",
    "    actions = actions.split('item {\\n  ')[1:]\n",
    "    actions = [[keys.split(': ') for keys in ac.split('\\n')[:2]] for ac in actions]\n",
    "    actions_dict ={}\n",
    "    for ac in actions:\n",
    "        actions_dict[int(ac[1][1])] = ac[0][1][1:-1]\n",
    "    return actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthBbox(df):\n",
    "    bboxes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        bboxes.append([row['x1'], row['y1'], row['x2'], row['y2'], row['action_id']])\n",
    "    return np.array(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_groundTruth_bboxes(image, bboxes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    actions = buildActionDict()\n",
    "    for bbox in bboxes:\n",
    "        top_left = (int(bbox[0]), int(bbox[1]))\n",
    "        bottom_right = (int(bbox[2]), int(bbox[3]))\n",
    "        action_id = bbox[4]\n",
    "        bbox_color = (255, 0, 255)\n",
    "        font_size = 0.4\n",
    "        font_thickness = 1\n",
    "        cv2.rectangle(image, top_left, bottom_right, bbox_color, 2)\n",
    "        bbox_text = actions[action_id]\n",
    "        t_size = cv2.getTextSize(bbox_text, 0, font_size, font_thickness)[0]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            top_left,\n",
    "            (top_left[0] + t_size[0], top_left[1] - t_size[1] - 3),\n",
    "            bbox_color,\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            bbox_text,\n",
    "            (top_left[0], top_left[1] - 2),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_size,\n",
    "            (255 - bbox_color[0], 255 - bbox_color[1], 255 - bbox_color[2]),\n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects(image, bboxes, classes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    person_count = 0\n",
    "    for bbox in bboxes:\n",
    "        c_x = int(bbox[0])\n",
    "        c_y = int(bbox[1])\n",
    "        half_w = int(bbox[2] / 2)\n",
    "        half_h = int(bbox[3] / 2)\n",
    "        top_left = [c_x - half_w, c_y - half_h]\n",
    "        bottom_right = [c_x + half_w, c_y + half_h]\n",
    "        top_left[0] = max(top_left[0], 0)\n",
    "        top_left[1] = max(top_left[1], 0)\n",
    "        bottom_right[0] = min(bottom_right[0], width)\n",
    "        bottom_right[1] = min(bottom_right[1], height)\n",
    "        class_id = int(bbox[4])\n",
    "        if class_id == 0:\n",
    "            person_count += 1\n",
    "            windowName = \"{}_{}\".format(classes[class_id],person_count)\n",
    "            cv2.namedWindow(windowName, cv2.WINDOW_AUTOSIZE)\n",
    "            obj = image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0], :]\n",
    "            cv2.imshow(windowName, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildYoloModel():\n",
    "    yolo = YOLOv4()\n",
    "    yolo.classes = \"coco.names\"\n",
    "    yolo.input_size=(608,608)\n",
    "    yolo.make_model()\n",
    "    yolo.load_weights(\"yolov4.weights\", weights_type='yolo')\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(media_path, yolo, groundTruth_df, iou_threshold = 0.5, score_threshold = 0.5, start_time = 902, end_time = 1798):\n",
    "    \n",
    "    if not os.path.exists(media_path):\n",
    "        raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
    "\n",
    "    cv2.namedWindow(\"result\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"origin\", cv2.WINDOW_AUTOSIZE)\n",
    "    #cv2.namedWindow(\"ground_truth\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    cap = cv2.VideoCapture(media_path)\n",
    "    \n",
    "    tracker = Tracker()\n",
    "    action_freq = 8\n",
    "    W, H = int(cap.get(3)), int(cap.get(4))\n",
    "    T = tracker.timesteps\n",
    "    act_detector = act.Action_Detector('soft_attn')\n",
    "    ckpt_name = 'model_ckpt_soft_attn_pooled_cosine_drop_ava-130'\n",
    "    memory_size = act_detector.timesteps - action_freq\n",
    "    updated_frames, temporal_rois, temporal_roi_batch_indices, cropped_frames = act_detector.crop_tubes_in_tf_with_memory([T,H,W,3], memory_size)\n",
    "    \n",
    "    rois, roi_batch_indices, pred_probs = act_detector.define_inference_with_placeholders_noinput(cropped_frames)\n",
    "    \n",
    "    ckpt_path = os.path.join('./', 'action_detection', 'weights', ckpt_name)\n",
    "    act_detector.restore_model(ckpt_path)\n",
    "\n",
    "    prob_dict = {}\n",
    "    \n",
    "    frame_cnt = 0\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            try:\n",
    "                is_success, frame = cap.read()\n",
    "            except cv2.error:\n",
    "                continue\n",
    "                \n",
    "            now_second = cap.get(0)/1000\n",
    "            \n",
    "            if now_second < start_time: continue\n",
    "            if (not is_success) or (now_second >= end_time+1): break\n",
    "                \n",
    "            frame_cnt += 1\n",
    "            print(frame_cnt)\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            bboxes = yolo.predict(\n",
    "                frame,\n",
    "                iou_threshold=iou_threshold,\n",
    "                score_threshold=score_threshold,\n",
    "            )\n",
    "            bboxes.view('i8,i8,i8,i8,i8,i8').sort(order=['f0','f1'], axis=0)\n",
    "            \n",
    "            tracker.update_tracker(bboxes, frame)\n",
    "            no_actors = len(tracker.active_actors)\n",
    "            \n",
    "            if tracker.active_actors and frame_cnt % action_freq == 0:\n",
    "                probs = []\n",
    "\n",
    "                cur_input_sequence = np.expand_dims(np.stack(tracker.frame_history[-action_freq:], axis=0), axis=0)\n",
    "\n",
    "                rois_np, temporal_rois_np = tracker.generate_all_rois()\n",
    "                if no_actors > 14:\n",
    "                    no_actors = 14\n",
    "                    rois_np = rois_np[:14]\n",
    "                    temporal_rois_np = temporal_rois_np[:14]\n",
    "\n",
    "                feed_dict = {updated_frames:cur_input_sequence, # only update last #action_freq frames\n",
    "                             temporal_rois: temporal_rois_np,\n",
    "                             temporal_roi_batch_indices: np.zeros(no_actors),\n",
    "                             rois:rois_np, \n",
    "                             roi_batch_indices:np.arange(no_actors)}\n",
    "                run_dict = {'pred_probs': pred_probs}\n",
    "                out_dict = act_detector.session.run(run_dict, feed_dict=feed_dict)\n",
    "                probs = out_dict['pred_probs']\n",
    "                # associate probs with actor ids\n",
    "                print_top_k = 5\n",
    "                for bb in range(no_actors):\n",
    "                    act_probs = probs[bb]\n",
    "                    order = np.argsort(act_probs)[::-1]\n",
    "                    cur_actor_id = tracker.active_actors[bb]['actor_id']\n",
    "                    print(\"Person %i\" % cur_actor_id)\n",
    "                    cur_results = []\n",
    "                    for pp in range(print_top_k):\n",
    "                        print('\\t %s: %.3f' % (act.ACTION_STRINGS[order[pp]], act_probs[order[pp]]))\n",
    "                        cur_results.append((act.ACTION_STRINGS[order[pp]], act_probs[order[pp]]))\n",
    "                    prob_dict[cur_actor_id] = cur_results\n",
    "\n",
    "                t5 = time.time(); print('action %.2f seconds' % (t5-t3))\n",
    "            \n",
    "            #groundTruth_bboxes = getGroundTruthBbox(groundTruth_df[groundTruth_df['timestamp']==int(now_second)])\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            image = yolo.draw_bboxes(frame, bboxes)\n",
    "            #groundTruth_img = draw_groundTruth_bboxes(frame, groundTruth_bboxes)\n",
    "\n",
    "            cv2.imshow(\"result\", image)\n",
    "            cv2.imshow(\"origin\", frame)\n",
    "            #cv2.imshow(\"ground_truth\", groundTruth_img)\n",
    "            #draw_objects(frame, bboxes, yolo.classes)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectDetection(path, media_name, yolo, iou_threshold = 0.5, score_threshold = 0.5, start_time = 902, end_time = 1798):\n",
    "    \n",
    "    media_path = path + media_name\n",
    "    \n",
    "    if not os.path.exists(media_path):\n",
    "        raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
    "\n",
    "    cap = cv2.VideoCapture(media_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            try:\n",
    "                is_success, frame = cap.read()\n",
    "            except cv2.error:\n",
    "                continue\n",
    "                \n",
    "            now_second = cap.get(0)/1000\n",
    "            \n",
    "            if now_second < start_time: continue\n",
    "            if (not is_success) or (now_second >= end_time+1): break\n",
    "                \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            bboxes = yolo.predict(\n",
    "                frame,\n",
    "                iou_threshold=iou_threshold,\n",
    "                score_threshold=score_threshold,\n",
    "            )\n",
    "            bboxes.view('i8,i8,i8,i8,i8,i8').sort(order=['f0','f1'], axis=0)\n",
    "            for bb in bboxes:\n",
    "                if bb[4] == 0:\n",
    "                    obj = [media_name, now_second]+list(bb)\n",
    "                    objs.append(obj)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ava_videos/ava_file_names_trainval_v2.1.txt', 'r') as f:\n",
    "    video_names = f.readlines()\n",
    "video_names = [v.rstrip().split('.') for v in video_names]\n",
    "video_names_dict = {}\n",
    "for video in video_names:\n",
    "    video_names_dict[video[0]] = video[0]+'.'+video[1]\n",
    "\n",
    "columns = ['video_id', 'timestamp', 'x1', 'y1', 'x2', 'y2', 'action_id', 'person_id']\n",
    "train_df = pd.read_csv('ava_videos/ava_train_v2.2.csv')\n",
    "val_df = pd.read_csv('ava_videos/ava_val_v2.2.csv')\n",
    "train_df.columns = columns\n",
    "val_df.columns = columns\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "train_df.drop(train_df[train_df['video_id']=='#NAME?'].index, inplace=True)\n",
    "train_df['video_id'] = train_df['video_id'].map(video_names_dict)\n",
    "val_df['video_id'] = val_df['video_id'].map(video_names_dict)\n",
    "\n",
    "train_videos = train_df['video_id'].unique()\n",
    "val_videos = val_df['video_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1 , Processing: 1j20qq1JyX4.mp4\n"
     ]
    }
   ],
   "source": [
    "yolo = buildYoloModel()\n",
    "\n",
    "train_path = \"ava_videos/train/\"\n",
    "val_path = \"ava_videos/val/\"\n",
    "\n",
    "cnt = 0\n",
    "for media_name in val_videos:\n",
    "    start = time.time()\n",
    "    objs = []\n",
    "    cnt+=1\n",
    "    print(\"Video\", cnt, \", Processing:\", media_name)\n",
    "    objectDetection(val_path, media_name, yolo)\n",
    "    print(\"Done in \", time.time()-start, 'seconds')\n",
    "    res = pd.DataFrame(objs, columns=['video_id','timestamp','c_x','c_y','w','h','obj_id','confidence'])\n",
    "    res.to_csv(media_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64625376 0.52953063 0.56066054 0.9593384  0.         0.95671797]]\n",
      "[[0.64709496 0.52929073 0.55817723 0.96114922 0.         0.96396029]]\n",
      "[[0.64737463 0.52802891 0.55543369 0.95834234 0.         0.96096146]]\n",
      "[[0.647035   0.52766768 0.55654144 0.95937453 0.         0.95982504]]\n",
      "[[0.6469717  0.52754074 0.5589447  0.96170722 0.         0.95807731]]\n",
      "[[0.64701718 0.52735329 0.55822039 0.96316825 0.         0.95761442]]\n",
      "[[0.64754409 0.52748617 0.55817181 0.95704492 0.         0.95818079]]\n",
      "[[0.64795047 0.52818341 0.56000203 0.95656247 0.         0.96046382]]\n",
      "[[0.64860839 0.52779526 0.55726939 0.9577677  0.         0.96552509]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-63d9660666ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedia_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmedia_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-376608749776>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(media_path, yolo, groundTruth_df, iou_threshold, score_threshold, start_time, end_time)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             )\n\u001b[1;32m     27\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8,i8,i8,i8,i8,i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/yolov4/tf/__init__.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, frame, iou_threshold, score_threshold)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Select 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(media_path, yolo, train_df[train_df['video_id']==media_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.executing_eagerly(), \"Sonnet v2 requires TensorFlow 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'bend/bow (at the waist)',\n",
       " 3: 'crouch/kneel',\n",
       " 4: 'dance',\n",
       " 5: 'fall down',\n",
       " 6: 'get up',\n",
       " 7: 'jump/leap',\n",
       " 8: 'lie/sleep',\n",
       " 9: 'martial art',\n",
       " 10: 'run/jog',\n",
       " 11: 'sit',\n",
       " 12: 'stand',\n",
       " 13: 'swim',\n",
       " 14: 'walk',\n",
       " 15: 'answer phone',\n",
       " 17: 'carry/hold (an object)',\n",
       " 20: 'climb (e.g., a mountain)',\n",
       " 22: 'close (e.g., a door, a box)',\n",
       " 24: 'cut',\n",
       " 26: 'dress/put on clothing',\n",
       " 27: 'drink',\n",
       " 28: 'drive (e.g., a car, a truck)',\n",
       " 29: 'eat',\n",
       " 30: 'enter',\n",
       " 34: 'hit (an object)',\n",
       " 36: 'lift/pick up',\n",
       " 37: 'listen (e.g., to music)',\n",
       " 38: 'open (e.g., a window, a car door)',\n",
       " 41: 'play musical instrument',\n",
       " 43: 'point to (an object)',\n",
       " 45: 'pull (an object)',\n",
       " 46: 'push (an object)',\n",
       " 47: 'put down',\n",
       " 48: 'read',\n",
       " 49: 'ride (e.g., a bike, a car, a horse)',\n",
       " 51: 'sail boat',\n",
       " 52: 'shoot',\n",
       " 54: 'smoke',\n",
       " 56: 'take a photo',\n",
       " 57: 'text on/look at a cellphone',\n",
       " 58: 'throw',\n",
       " 59: 'touch (an object)',\n",
       " 60: 'turn (e.g., a screwdriver)',\n",
       " 61: 'watch (e.g., TV)',\n",
       " 62: 'work on a computer',\n",
       " 63: 'write',\n",
       " 64: 'fight/hit (a person)',\n",
       " 65: 'give/serve (an object) to (a person)',\n",
       " 66: 'grab (a person)',\n",
       " 67: 'hand clap',\n",
       " 68: 'hand shake',\n",
       " 69: 'hand wave',\n",
       " 70: 'hug (a person)',\n",
       " 72: 'kiss (a person)',\n",
       " 73: 'lift (a person)',\n",
       " 74: 'listen to (a person)',\n",
       " 76: 'push (another person)',\n",
       " 77: 'sing to (e.g., self, a person, a group)',\n",
       " 78: 'take (an object) from (a person)',\n",
       " 79: 'talk to (e.g., self, a person, a group)',\n",
       " 80: 'watch (a person)'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildActionDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# Some modules to help with reading the UCF101 dataset.\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import ssl\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "from urllib import request  # requires python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to fetch videos from UCF101 dataset\n",
    "UCF_ROOT = \"https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/\"\n",
    "_VIDEO_LIST = None\n",
    "_CACHE_DIR = tempfile.mkdtemp()\n",
    "# As of July 2020, crcv.ucf.edu doesn't use a certificate accepted by the\n",
    "# default Colab environment anymore.\n",
    "unverified_context = ssl._create_unverified_context()\n",
    "\n",
    "def list_ucf_videos():\n",
    "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
    "  global _VIDEO_LIST\n",
    "  if not _VIDEO_LIST:\n",
    "    index = request.urlopen(UCF_ROOT, context=unverified_context).read().decode(\"utf-8\")\n",
    "    videos = re.findall(\"(v_[\\w_]+\\.avi)\", index)\n",
    "    _VIDEO_LIST = sorted(set(videos))\n",
    "  return list(_VIDEO_LIST)\n",
    "\n",
    "def fetch_ucf_video(video):\n",
    "  \"\"\"Fetchs a video and cache into local filesystem.\"\"\"\n",
    "  cache_path = os.path.join(_CACHE_DIR, video)\n",
    "  if not os.path.exists(cache_path):\n",
    "    urlpath = request.urljoin(UCF_ROOT, video)\n",
    "    print(\"Fetching %s => %s\" % (urlpath, cache_path))\n",
    "    data = request.urlopen(urlpath, context=unverified_context).read()\n",
    "    open(cache_path, \"wb\").write(data)\n",
    "  return cache_path\n",
    "\n",
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "  y, x = frame.shape[0:2]\n",
    "  min_dim = min(y, x)\n",
    "  start_x = (x // 2) - (min_dim // 2)\n",
    "  start_y = (y // 2) - (min_dim // 2)\n",
    "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(224, 224)):\n",
    "  start_time = 902\n",
    "  end_time = 907\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  frame_width = int(cap.get(3)) \n",
    "  frame_height = int(cap.get(4)) \n",
    "   \n",
    "  size = (frame_width, frame_height) \n",
    "  result = cv2.VideoWriter('test.avi',  \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'), \n",
    "                         30, size) \n",
    "  frames = []\n",
    "  try:\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      now_second = cap.get(0)/1000\n",
    "      if now_second < start_time: continue\n",
    "      if (now_second >= end_time+1): break\n",
    "      result.write(frame)\n",
    "      frame = crop_center_square(frame)\n",
    "      frame = cv2.resize(frame, resize)\n",
    "      frame = frame[:, :, [2, 1, 0]]\n",
    "      frames.append(frame)\n",
    "\n",
    "      if len(frames) == max_frames:\n",
    "        break\n",
    "  finally:\n",
    "    cap.release()\n",
    "  return np.array(frames) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i3d = hub.load(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\").signatures['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample_video):\n",
    "  # Add a batch axis to the to the sample video.\n",
    "  model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n",
    "\n",
    "  logits = i3d(model_input)['default'][0]\n",
    "  probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "  print(\"Top 5 actions:\")\n",
    "  for i in np.argsort(probabilities)[::-1][:5]:\n",
    "    print(f\"  {labels[i]:22}: {probabilities[i] * 100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_path = fetch_ucf_video(\"v_CricketShot_g04_c02.avi\")\n",
    "sample_video = load_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 labels.\n"
     ]
    }
   ],
   "source": [
    "# Get the kinetics-400 action labels from the GitHub repository.\n",
    "KINETICS_URL = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
    "with request.urlopen(KINETICS_URL) as obj:\n",
    "  labels = [line.decode(\"utf-8\").strip() for line in obj.readlines()]\n",
    "print(\"Found %d labels.\" % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_media = load_video(media_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 actions:\n",
      "  shaving legs          :  9.57%\n",
      "  dancing ballet        :  8.78%\n",
      "  plastering            :  5.42%\n",
      "  stretching arm        :  4.80%\n",
      "  yawning               :  4.43%\n"
     ]
    }
   ],
   "source": [
    "predict(sample_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def inference(\n",
      "        self,\n",
      "        media_path,\n",
      "        is_image: bool = True,\n",
      "        cv_apiPreference=None,\n",
      "        cv_frame_size: tuple = None,\n",
      "        cv_fourcc: str = None,\n",
      "        cv_waitKey_delay: int = 1,\n",
      "        iou_threshold: float = 0.3,\n",
      "        score_threshold: float = 0.25,\n",
      "    ):\n",
      "        if not path.exists(media_path):\n",
      "            raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
      "\n",
      "        cv2.namedWindow(\"result\", cv2.WINDOW_AUTOSIZE)\n",
      "\n",
      "        if is_image:\n",
      "            frame = cv2.imread(media_path)\n",
      "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "            start_time = time.time()\n",
      "            bboxes = self.predict(\n",
      "                frame,\n",
      "                iou_threshold=iou_threshold,\n",
      "                score_threshold=score_threshold,\n",
      "            )\n",
      "            exec_time = time.time() - start_time\n",
      "            print(\"time: {:.2f} ms\".format(exec_time * 1000))\n",
      "\n",
      "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
      "            image = self.draw_bboxes(frame, bboxes)\n",
      "            cv2.imshow(\"result\", image)\n",
      "        else:\n",
      "            if cv_apiPreference is None:\n",
      "                cap = cv2.VideoCapture(media_path)\n",
      "            else:\n",
      "                cap = cv2.VideoCapture(media_path, cv_apiPreference)\n",
      "\n",
      "            if cv_frame_size is not None:\n",
      "                cap.set(cv2.CAP_PROP_FRAME_WIDTH, cv_frame_size[0])\n",
      "                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cv_frame_size[1])\n",
      "\n",
      "            if cv_fourcc is not None:\n",
      "                cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*cv_fourcc))\n",
      "\n",
      "            prev_time = time.time()\n",
      "            if cap.isOpened():\n",
      "                while True:\n",
      "                    try:\n",
      "                        is_success, frame = cap.read()\n",
      "                    except cv2.error:\n",
      "                        continue\n",
      "\n",
      "                    if not is_success:\n",
      "                        break\n",
      "\n",
      "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "                    predict_start_time = time.time()\n",
      "                    bboxes = self.predict(\n",
      "                        frame,\n",
      "                        iou_threshold=iou_threshold,\n",
      "                        score_threshold=score_threshold,\n",
      "                    )\n",
      "                    predict_exec_time = time.time() - predict_start_time\n",
      "\n",
      "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
      "                    image = self.draw_bboxes(frame, bboxes)\n",
      "                    curr_time = time.time()\n",
      "\n",
      "                    cv2.putText(\n",
      "                        image,\n",
      "                        \"preidct: {:.2f} ms, fps: {:.2f}\".format(\n",
      "                            predict_exec_time * 1000,\n",
      "                            1 / (curr_time - prev_time),\n",
      "                        ),\n",
      "                        org=(5, 20),\n",
      "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
      "                        fontScale=0.6,\n",
      "                        color=(50, 255, 0),\n",
      "                        thickness=2,\n",
      "                        lineType=cv2.LINE_AA,\n",
      "                    )\n",
      "                    prev_time = curr_time\n",
      "\n",
      "                    cv2.imshow(\"result\", image)\n",
      "                    if cv2.waitKey(cv_waitKey_delay) & 0xFF == ord(\"q\"):\n",
      "                        break\n",
      "\n",
      "            cap.release()\n",
      "\n",
      "        print(\"YOLOv4: Inference is finished\")\n",
      "        while cv2.waitKey(10) & 0xFF != ord(\"q\"):\n",
      "            pass\n",
      "        cv2.destroyWindow(\"result\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(yolo.inference)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
