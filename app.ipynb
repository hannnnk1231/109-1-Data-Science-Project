{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "#from yolov4.tf import YOLOv4\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hank/.local/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.tools.generate_detections import create_box_encoder\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker as ds_Tracker\n",
    "MODEL_CKPT = \"./deep_sort/weights/mars-small128.pb\"\n",
    "import action_detection.action_detector as act\n",
    "COLORS = np.random.randint(0, 255, [1000, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker():\n",
    "    def __init__(self, timesteps=32):\n",
    "        self.active_actors = []\n",
    "        self.inactive_actors = []\n",
    "        self.actor_no = 0\n",
    "        self.frame_history = []\n",
    "        self.frame_no = 0\n",
    "        self.timesteps = timesteps\n",
    "        self.actor_infos = {}\n",
    "        # deep sort\n",
    "        self.encoder = create_box_encoder(MODEL_CKPT, batch_size=16)\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.2, None) #, max_cosine_distance=0.2) #, nn_budget=None)\n",
    "        self.tracker = ds_Tracker(metric, max_iou_distance=0.7, max_age=200, n_init=5)\n",
    "        self.score_th = 0.40\n",
    "\n",
    "    def update_tracker(self, detection_info, frame):\n",
    "        ''' Takes the frame and the results from the object detection\n",
    "            Updates the tracker wwith the current detections and creates new tracks\n",
    "        '''\n",
    "\n",
    "        boxes = np.array([d[:4] for d in detection_info])\n",
    "        scores = np.array([d[4] for d in detection_info])\n",
    "        num_detections = len(detection_info)\n",
    "        indices = scores > self.score_th # filter score threshold\n",
    "        filtered_boxes, filtered_scores = boxes[indices], scores[indices]\n",
    "\n",
    "        H,W,C = frame.shape\n",
    "        filtered_boxes[:, [0, 2]] = filtered_boxes[:, [0, 2]] * W\n",
    "        filtered_boxes[:, [1, 3]] = filtered_boxes[:, [1, 3]] * H\n",
    "        # deep sort format boxes (x, y, W, H)\n",
    "        ds_boxes = []\n",
    "        for bb in range(filtered_boxes.shape[0]):\n",
    "            cur_box = filtered_boxes[bb]\n",
    "            cur_score = filtered_scores[bb]\n",
    "            c_x = int(cur_box[0])\n",
    "            c_y = int(cur_box[1])\n",
    "            half_w = int(cur_box[2]/2)\n",
    "            half_h = int(cur_box[3]/2)\n",
    "            ds_box = [c_x - half_w, c_y - half_h, int(cur_box[2]), int(cur_box[3])]\n",
    "            ds_boxes.append(ds_box)\n",
    "        features = self.encoder(frame, ds_boxes)\n",
    "\n",
    "        detection_list = []\n",
    "        for bb in range(filtered_boxes.shape[0]):\n",
    "            cur_box = filtered_boxes[bb]\n",
    "            cur_score = filtered_scores[bb]\n",
    "            feature = features[bb]\n",
    "            c_x = int(cur_box[0])\n",
    "            c_y = int(cur_box[1])\n",
    "            half_w = int(cur_box[2]/2)\n",
    "            half_h = int(cur_box[3]/2)\n",
    "            ds_box = [c_x - half_w, c_y - half_h, int(cur_box[2]), int(cur_box[3])]\n",
    "            detection_list.append(Detection(ds_box, cur_score, feature))\n",
    "\n",
    "        # update tracker\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(detection_list)\n",
    "        \n",
    "        # Store results.\n",
    "        actives = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlwh()\n",
    "            left, top, width, height = bbox\n",
    "            tr_box = [top / float(H), left / float(W), (top+height)/float(H), (left+width)/float(W)]\n",
    "            actor_id = track.track_id\n",
    "            detection_conf = track.last_detection_confidence\n",
    "            \n",
    "            if actor_id in self.actor_infos: # update with the new bbox info\n",
    "                cur_actor = self.actor_infos[actor_id]\n",
    "                no_interpolate_frames = self.frame_no - cur_actor['last_updated_frame_no']\n",
    "                interpolated_box_list = bbox_interpolate(cur_actor['all_boxes'][-1], tr_box, no_interpolate_frames)\n",
    "                cur_actor['all_boxes'].extend(interpolated_box_list[1:])\n",
    "                cur_actor['last_updated_frame_no'] = self.frame_no\n",
    "                cur_actor['length'] = len(cur_actor['all_boxes'])\n",
    "                cur_actor['all_scores'].append(detection_conf)\n",
    "                actives.append(cur_actor)\n",
    "            else:\n",
    "                new_actor = {'all_boxes': [tr_box], 'length':1, 'last_updated_frame_no': self.frame_no, 'all_scores':[detection_conf], 'actor_id':actor_id}\n",
    "                self.actor_infos[actor_id] = new_actor\n",
    "\n",
    "        self.active_actors = actives\n",
    "        \n",
    "        self.frame_history.append(frame)\n",
    "        if len(self.frame_history) > 2*self.timesteps:\n",
    "            del self.frame_history[0]\n",
    "\n",
    "        self.frame_no += 1\n",
    "\n",
    "    def generate_all_rois(self):\n",
    "        no_actors = len(self.active_actors)\n",
    "        rois_np = np.zeros([no_actors, 4])\n",
    "        temporal_rois_np = np.zeros([no_actors, self.timesteps, 4])\n",
    "        for bb, actor_info in enumerate(self.active_actors):\n",
    "            actor_no = actor_info['actor_id']\n",
    "            norm_roi, full_roi = self.generate_person_tube_roi(actor_no)\n",
    "            rois_np[bb] = norm_roi\n",
    "            temporal_rois_np[bb] = full_roi\n",
    "        return rois_np, temporal_rois_np\n",
    "\n",
    "    def generate_person_tube_roi(self, actor_id):\n",
    "        actor_info = [act for act in self.active_actors if act['actor_id'] == actor_id][0]\n",
    "        boxes = actor_info['all_boxes']\n",
    "        if actor_info['length'] < self.timesteps:\n",
    "            recent_boxes = boxes\n",
    "            index_offset = (self.timesteps - actor_info['length'] + 1) \n",
    "        else:\n",
    "            recent_boxes = boxes[-self.timesteps:]\n",
    "            index_offset = 0\n",
    "        H,W,C = self.frame_history[-1].shape\n",
    "        mid_box = recent_boxes[len(recent_boxes)//2]\n",
    "        edge, norm_roi = generate_edge_and_normalized_roi(mid_box)\n",
    "\n",
    "        full_rois = []\n",
    "        for rr in range(self.timesteps):\n",
    "            if rr < index_offset:\n",
    "                cur_box = recent_boxes[0]\n",
    "            else:\n",
    "                cur_box = recent_boxes[rr - index_offset]\n",
    "\n",
    "            top, left, bottom, right = cur_box\n",
    "            cur_center = (top+bottom)/2., (left+right)/2.\n",
    "            top, bottom = cur_center[0] - edge, cur_center[0] + edge\n",
    "            left, right = cur_center[1] - edge, cur_center[1] + edge\n",
    "            \n",
    "            full_rois.append([top, left, bottom, right])\n",
    "        full_rois_np = np.stack(full_rois, axis=0)\n",
    "\n",
    "        return norm_roi, full_rois_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_interpolate(start_box, end_box, no_interpolate_frames):\n",
    "    delta = (np.array(end_box) - np.array(start_box)) / float(no_interpolate_frames)\n",
    "    interpolated_boxes = []\n",
    "    for ii in range(0, no_interpolate_frames+1):\n",
    "        cur_box = np.array(start_box) + delta * ii\n",
    "        interpolated_boxes.append(cur_box.tolist())\n",
    "    return interpolated_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_and_normalized_roi(mid_box):\n",
    "    top, left, bottom, right = mid_box\n",
    "\n",
    "    edge = max(bottom - top, right - left) / 2. * 1.5 # change this to change the size of the tube\n",
    "\n",
    "    cur_center = (top+bottom)/2., (left+right)/2.\n",
    "    context_top, context_bottom = cur_center[0] - edge, cur_center[0] + edge\n",
    "    context_left, context_right = cur_center[1] - edge, cur_center[1] + edge\n",
    "\n",
    "    normalized_top = (top - context_top) / (2*edge)\n",
    "    normalized_bottom = (bottom - context_top) / (2*edge)\n",
    "\n",
    "    normalized_left = (left - context_left) / (2*edge)\n",
    "    normalized_right = (right - context_left) / (2*edge)\n",
    "\n",
    "    norm_roi = [normalized_top, normalized_left, normalized_bottom, normalized_right]\n",
    "\n",
    "    return edge, norm_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildActionDict():\n",
    "    with open(\"ava_videos/action_list.pbtxt\", 'r') as file:\n",
    "        actions = file.read()\n",
    "    actions = actions.split('item {\\n  ')[1:]\n",
    "    actions = [[keys.split(': ') for keys in ac.split('\\n')[:2]] for ac in actions]\n",
    "    actions_dict ={}\n",
    "    for ac in actions:\n",
    "        actions_dict[int(ac[1][1])] = ac[0][1][1:-1]\n",
    "    return actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthBbox(df):\n",
    "    bboxes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        bboxes.append([row['x1'], row['y1'], row['x2'], row['y2'], row['action_id']])\n",
    "    return np.array(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_groundTruth_bboxes(image, bboxes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    actions = buildActionDict()\n",
    "    for bbox in bboxes:\n",
    "        top_left = (int(bbox[0]), int(bbox[1]))\n",
    "        bottom_right = (int(bbox[2]), int(bbox[3]))\n",
    "        action_id = bbox[4]\n",
    "        bbox_color = (255, 0, 255)\n",
    "        font_size = 0.4\n",
    "        font_thickness = 1\n",
    "        cv2.rectangle(image, top_left, bottom_right, bbox_color, 2)\n",
    "        bbox_text = actions[action_id]\n",
    "        t_size = cv2.getTextSize(bbox_text, 0, font_size, font_thickness)[0]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            top_left,\n",
    "            (top_left[0] + t_size[0], top_left[1] - t_size[1] - 3),\n",
    "            bbox_color,\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            bbox_text,\n",
    "            (top_left[0], top_left[1] - 2),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_size,\n",
    "            (255 - bbox_color[0], 255 - bbox_color[1], 255 - bbox_color[2]),\n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects(image, bboxes, classes):\n",
    "    image = np.copy(image)\n",
    "    height, width, _ = image.shape\n",
    "    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] * width\n",
    "    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] * height\n",
    "    person_count = 0\n",
    "    for bbox in bboxes:\n",
    "        c_x = int(bbox[0])\n",
    "        c_y = int(bbox[1])\n",
    "        half_w = int(bbox[2] / 2)\n",
    "        half_h = int(bbox[3] / 2)\n",
    "        top_left = [c_x - half_w, c_y - half_h]\n",
    "        bottom_right = [c_x + half_w, c_y + half_h]\n",
    "        top_left[0] = max(top_left[0], 0)\n",
    "        top_left[1] = max(top_left[1], 0)\n",
    "        bottom_right[0] = min(bottom_right[0], width)\n",
    "        bottom_right[1] = min(bottom_right[1], height)\n",
    "        class_id = int(bbox[4])\n",
    "        if class_id == 0:\n",
    "            person_count += 1\n",
    "            windowName = \"{}_{}\".format(classes[class_id],person_count)\n",
    "            cv2.namedWindow(windowName, cv2.WINDOW_AUTOSIZE)\n",
    "            obj = image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0], :]\n",
    "            cv2.imshow(windowName, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildYoloModel():\n",
    "    yolo = YOLOv4()\n",
    "    yolo.classes = \"coco.names\"\n",
    "    yolo.input_size=(608,608)\n",
    "    yolo.make_model()\n",
    "    yolo.load_weights(\"yolov4.weights\", weights_type='yolo')\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectDetection(path, media_name, yolo, iou_threshold = 0.5, score_threshold = 0.5, start_time = 902, end_time = 1798):\n",
    "    \n",
    "    media_path = path + media_name\n",
    "    \n",
    "    if not os.path.exists(media_path):\n",
    "        raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
    "\n",
    "    cap = cv2.VideoCapture(media_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            try:\n",
    "                is_success, frame = cap.read()\n",
    "            except cv2.error:\n",
    "                continue\n",
    "                \n",
    "            now_second = cap.get(0)/1000\n",
    "            \n",
    "            if now_second < start_time: continue\n",
    "            if (not is_success) or (now_second >= end_time+1): break\n",
    "                \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            bboxes = yolo.predict(\n",
    "                frame,\n",
    "                iou_threshold=iou_threshold,\n",
    "                score_threshold=score_threshold,\n",
    "            )\n",
    "            bboxes.view('i8,i8,i8,i8,i8,i8').sort(order=['f0','f1'], axis=0)\n",
    "            for bb in bboxes:\n",
    "                if bb[4] == 0:\n",
    "                    obj = [media_name, now_second]+list(bb)\n",
    "                    objs.append(obj)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuildBBoxes(media_name):\n",
    "    bbox_df = pd.read_csv('bbox_res/'+media_name+'.csv')\n",
    "    cur_timestamp = 0\n",
    "    bboxes = []\n",
    "    for index, row in bbox_df.iterrows():\n",
    "        if row['timestamp'] != cur_timestamp:\n",
    "            bboxes.append([])\n",
    "            cur_timestamp = row['timestamp']    \n",
    "        bboxes[-1].append([row['c_x'],row['c_y'],row['w'],row['h'],row['confidence']])\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_detection_results(active_actors, prob_dict, timestamp, media_name):\n",
    "    for ii in range(len(active_actors)):\n",
    "        cur_actor = active_actors[ii]\n",
    "        actor_id = cur_actor['actor_id']\n",
    "        cur_act_results = prob_dict[actor_id] if actor_id in prob_dict else []\n",
    "        try:\n",
    "            cur_box, cur_score, cur_class = cur_actor['all_boxes'][-16], cur_actor['all_scores'][0], 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "        act_dict = buildActionDict()\n",
    "        act_dict = {v: k for k, v in act_dict.items()}\n",
    "        top, left, bottom, right = cur_box\n",
    "        for act in cur_act_results:\n",
    "            res_list.append([media_name, timestamp, left, top, right, bottom, act_dict[act[0]], actor_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection_results(img_np, active_actors, prob_dict):\n",
    "    \n",
    "    score_th = 0.30\n",
    "    action_th = 0.20\n",
    "\n",
    "    # copy the original image first\n",
    "    disp_img = np.copy(img_np)\n",
    "    H, W, C = img_np.shape\n",
    "    font_thickness = 1\n",
    "    \n",
    "    for ii in range(len(active_actors)):\n",
    "        cur_actor = active_actors[ii]\n",
    "        actor_id = cur_actor['actor_id']\n",
    "        cur_act_results = prob_dict[actor_id] if actor_id in prob_dict else []\n",
    "        try:\n",
    "            cur_box, cur_score, cur_class = cur_actor['all_boxes'][-16], cur_actor['all_scores'][0], 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "        if cur_score < score_th: \n",
    "            continue\n",
    "\n",
    "        top, left, bottom, right = cur_box\n",
    "\n",
    "        left = int(W * left)\n",
    "        right = int(W * right)\n",
    "\n",
    "        top = int(H * top)\n",
    "        bottom = int(H * bottom)\n",
    "\n",
    "        conf = cur_score\n",
    "        label = 'person'\n",
    "        message = '%s_%i: %% %.2f' % (label, actor_id, conf)\n",
    "        action_message_list = [\"%s:%.3f\" % (actres[0], actres[1]) for actres in cur_act_results if actres[1]>action_th]\n",
    "\n",
    "        color = COLORS[actor_id]\n",
    "        color = (int(color[0]), int(color[1]), int(color[2]))\n",
    "        \n",
    "        cv2.rectangle(disp_img, (left,top), (right,bottom), color, 3)\n",
    "\n",
    "        font_size =  max(0.5,(right - left)/50.0/float(len(message)))\n",
    "        cv2.rectangle(disp_img, (left, top-int(font_size*40)), (right,top), color, -1)\n",
    "        cv2.putText(\n",
    "            disp_img, \n",
    "            message, \n",
    "            (left, top-12), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            font_size, \n",
    "            (255-color[0],255-color[1],255-color[2]), \n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        #action message writing\n",
    "        cv2.rectangle(disp_img, (left, top), (right,top+10*len(action_message_list)), color, -1)\n",
    "        for aa, action_message in enumerate(action_message_list):\n",
    "            offset = aa*10\n",
    "            cv2.putText(\n",
    "                disp_img, \n",
    "                action_message, \n",
    "                (left, top+5+offset), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                (255-color[0],255-color[1],255-color[2]), \n",
    "                font_thickness,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    return disp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(path, media_name, iou_threshold = 0.5, score_threshold = 0.5, start_time = 902, end_time = 1798):\n",
    "    \n",
    "    print(\"Processing:\", media_name)\n",
    "    \n",
    "    media_path = path+media_name\n",
    "    \n",
    "    if not os.path.exists(media_path):\n",
    "        raise FileNotFoundError(\"{} does not exist\".format(media_path))\n",
    "\n",
    "    cv2.namedWindow(\"result\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    bbox_list = rebuildBBoxes(media_name)\n",
    "\n",
    "    cap = cv2.VideoCapture(media_path)\n",
    "    fps = cap.get(5)\n",
    "    W, H = int(cap.get(3)), int(cap.get(4))\n",
    "    out_video = cv2.VideoWriter('res_video/'+media_name+'.avi', cv2.VideoWriter_fourcc(*'MJPG'), fps, (W,H))\n",
    "    \n",
    "    tracker = Tracker()\n",
    "    action_freq = 8\n",
    "    T = tracker.timesteps\n",
    "    act_detector = act.Action_Detector('soft_attn')\n",
    "    ckpt_name = 'model_ckpt_soft_attn_pooled_cosine_drop_ava-130'\n",
    "    memory_size = act_detector.timesteps - action_freq\n",
    "    updated_frames, temporal_rois, temporal_roi_batch_indices, cropped_frames = act_detector.crop_tubes_in_tf_with_memory([T,H,W,3], memory_size)\n",
    "    \n",
    "    rois, roi_batch_indices, pred_probs = act_detector.define_inference_with_placeholders_noinput(cropped_frames)\n",
    "    \n",
    "    ckpt_path = os.path.join('./', 'action_detection', 'weights', ckpt_name)\n",
    "    act_detector.restore_model(ckpt_path)\n",
    "\n",
    "    prob_dict = {}\n",
    "    \n",
    "    frame_cnt = 0\n",
    "    sec_list = []\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            try:\n",
    "                is_success, frame = cap.read()\n",
    "            except cv2.error:\n",
    "                continue\n",
    "                \n",
    "            now_second = cap.get(0)/1000\n",
    "            \n",
    "            if now_second < start_time: continue\n",
    "            if (not is_success) or (now_second >= end_time+1): break\n",
    "                \n",
    "            bboxes = bbox_list[frame_cnt]\n",
    "            sec_list.append(now_second)\n",
    "            frame_cnt += 1\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            tracker.update_tracker(bboxes, frame)\n",
    "            no_actors = len(tracker.active_actors)\n",
    "            \n",
    "            if tracker.active_actors and frame_cnt % action_freq == 0:\n",
    "                probs = []\n",
    "\n",
    "                cur_input_sequence = np.expand_dims(np.stack(tracker.frame_history[-action_freq:], axis=0), axis=0)\n",
    "\n",
    "                rois_np, temporal_rois_np = tracker.generate_all_rois()\n",
    "                if no_actors > 14:\n",
    "                    no_actors = 14\n",
    "                    rois_np = rois_np[:14]\n",
    "                    temporal_rois_np = temporal_rois_np[:14]\n",
    "\n",
    "                feed_dict = {updated_frames:cur_input_sequence, # only update last #action_freq frames\n",
    "                             temporal_rois: temporal_rois_np,\n",
    "                             temporal_roi_batch_indices: np.zeros(no_actors),\n",
    "                             rois:rois_np, \n",
    "                             roi_batch_indices:np.arange(no_actors)}\n",
    "                run_dict = {'pred_probs': pred_probs}\n",
    "                out_dict = act_detector.session.run(run_dict, feed_dict=feed_dict)\n",
    "                probs = out_dict['pred_probs']\n",
    "                # associate probs with actor ids\n",
    "                print_top_k = 5\n",
    "                for bb in range(no_actors):\n",
    "                    act_probs = probs[bb]\n",
    "                    order = np.argsort(act_probs)[::-1]\n",
    "                    cur_actor_id = tracker.active_actors[bb]['actor_id']\n",
    "                    #print(\"Person %i\" % cur_actor_id)\n",
    "                    cur_results = []\n",
    "                    for pp in range(print_top_k):\n",
    "                        #print('\\t %s: %.3f' % (act.ACTION_STRINGS[order[pp]], act_probs[order[pp]]))\n",
    "                        cur_results.append((act.ACTION_STRINGS[order[pp]], act_probs[order[pp]]))\n",
    "                    prob_dict[cur_actor_id] = cur_results\n",
    "\n",
    "            if frame_cnt > 16:\n",
    "                store_detection_results(tracker.active_actors, prob_dict, sec_list[-16], media_name)\n",
    "                out_img = visualize_detection_results(tracker.frame_history[-16], tracker.active_actors, prob_dict)\n",
    "                cv2.imshow('result', out_img[:,:,::-1])\n",
    "                out_video.write(out_img[:,:,::-1])\n",
    "                cv2.waitKey(10)\n",
    "\n",
    "                \n",
    "            #groundTruth_bboxes = getGroundTruthBbox(groundTruth_df[groundTruth_df['timestamp']==int(now_second)])\n",
    "\n",
    "            #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            #image = yolo.draw_bboxes(frame, bboxes)\n",
    "            #groundTruth_img = draw_groundTruth_bboxes(frame, groundTruth_bboxes)\n",
    "\n",
    "            #cv2.imshow(\"result\", image)\n",
    "            #cv2.imshow(\"origin\", frame)\n",
    "            #cv2.imshow(\"ground_truth\", groundTruth_img)\n",
    "            #draw_objects(frame, bboxes, yolo.classes)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ava_videos/ava_file_names_trainval_v2.1.txt', 'r') as f:\n",
    "    video_names = f.readlines()\n",
    "video_names = [v.rstrip().split('.') for v in video_names]\n",
    "video_names_dict = {}\n",
    "for video in video_names:\n",
    "    video_names_dict[video[0]] = video[0]+'.'+video[1]\n",
    "\n",
    "columns = ['video_id', 'timestamp', 'x1', 'y1', 'x2', 'y2', 'action_id', 'person_id']\n",
    "#train_df = pd.read_csv('ava_videos/ava_train_v2.2.csv')\n",
    "val_df = pd.read_csv('ava_videos/ava_val_v2.2.csv')\n",
    "#train_df.columns = columns\n",
    "val_df.columns = columns\n",
    "#train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "#train_df.drop(train_df[train_df['video_id']=='#NAME?'].index, inplace=True)\n",
    "#train_df['video_id'] = train_df['video_id'].map(video_names_dict)\n",
    "val_df['video_id'] = val_df['video_id'].map(video_names_dict)\n",
    "\n",
    "#train_videos = train_df['video_id'].unique()\n",
    "val_videos = val_df['video_id'].unique()\n",
    "\n",
    "#train_path = \"ava_videos/train/\"\n",
    "val_path = \"ava_videos/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo = buildYoloModel()\n",
    "#cnt = 0\n",
    "#for media_name in val_videos:\n",
    "#    start = time.time()\n",
    "#    print(\"Video\", cnt, \", Processing:\", media_name)\n",
    "#    objectDetection(val_path, media_name, yolo, start_time = 1546.7452)\n",
    "#    print(\"Done in \", time.time()-start, 'seconds')\n",
    "#    res = pd.DataFrame(objs, columns=['video_id','timestamp','c_x','c_y','w','h','obj_id','confidence'])\n",
    "#    res.to_csv('bbox_res/'+media_name+'.csv', index=False)\n",
    "#    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1j20qq1JyX4.mp4\n",
      "Using model soft_attn\n",
      "Loading weights from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130\n",
      "INFO:tensorflow:Restoring parameters from ./action_detection/weights/model_ckpt_soft_attn_pooled_cosine_drop_ava-130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hank/.local/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for media_name in val_videos[0:]:\n",
    "    if os.path.exists('bbox_res/'+media_name+'.csv'):\n",
    "        start = time.time()\n",
    "        res_list = []\n",
    "        run(val_path, media_name)\n",
    "        res = pd.DataFrame(res_list, columns=['video_id','timestamp','x1','y1','x2','y2','action_id','person_id'])\n",
    "        res.to_csv('act_res/'+media_name+'.csv', index=False)\n",
    "        print(\"Done in \", time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
